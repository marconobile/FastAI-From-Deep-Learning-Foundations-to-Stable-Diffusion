{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a4e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn,tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torchmetrics.classification import MulticlassAccuracy \n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import fastcore.all as fc\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "@inplace\n",
    "def transformi(b):\n",
    "    b[x] = [TF.to_tensor(o) for o in b[x]]\n",
    "    \n",
    "\n",
    "class SingleBatchCB(Callback):\n",
    "    order = 1\n",
    "    def after_batch(self, learn):\n",
    "        raise CancelFitException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n",
    "    if bias is None: # add bias if norm is not a BatchNormLayer\n",
    "        bias = not isinstance(norm, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\n",
    "    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias)]\n",
    "    if norm: \n",
    "        layers.append(norm(nf))\n",
    "    if act: \n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "# conv->LayerNorm->act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=None, norm=None):\n",
    "    # nfs layer dims chain\n",
    "    if nfs is None:\n",
    "        nfs = [1, 8, 16, 32, 64]\n",
    "    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n",
    "    return nn.Sequential(*layers, \n",
    "                         conv(nfs[-1], 10, act=None, norm=False, bias = True),\n",
    "                         nn.Flatten()).to(device)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "set_seed(42)\n",
    "metrics = MetricCB(MulticlassAccuracy(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'\n",
    "dsd = load_dataset(\"fashion_mnist\")\n",
    "bs = 1024\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_datasetDict(tds, bs)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a373c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean, xstd = xb.mean(), xb.std()\n",
    "xmean, xstd # no normally distributed!\n",
    "\n",
    "def normalize(b):\n",
    "    return (b[0]-xmean)/xstd, b[1]\n",
    "\n",
    "norm = BatchTransformCB(normalize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527032d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak = 0.1\n",
    "general_leaky_relu = partial(GeneralRelu, leak=leak, subtract=0.4)\n",
    "astats = ActivationStats(append_stats, fc.risinstance(GeneralRelu)) # get only GeneralRelu # ,\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), metrics, astats, norm] \n",
    "f_init_weights = partial(init_weights, leaky=leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7529fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = MomentumLearner(model, dls, F.cross_entropy, lr=0.4, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8181994",
   "metadata": {},
   "outputs": [],
   "source": [
    "astats.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e5ec7",
   "metadata": {},
   "source": [
    "# SGD\n",
    "Now we implement SGD from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, params, lr, weight_decay=0.):\n",
    "        '''\n",
    "        params: generator of model params\n",
    "        '''\n",
    "        params = list(params) # pytorch put params in groups divided in the generator (not sure about generator/group)\n",
    "        fc.store_attr()\n",
    "        self.batch_number = 0 # required/used to build Adam\n",
    "        \n",
    "    def step(self):\n",
    "        '''\n",
    "        first regularizer step\n",
    "        then opt step\n",
    "        this works only for sgd\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                self.regularizer_step(p)\n",
    "                self.opt_step(p)\n",
    "        self.batch_number += 1\n",
    "        \n",
    "    def opt_step(self, p):\n",
    "        # directly modify the weights instead of mofifying the grad\n",
    "        p -= p.grad * self.lr \n",
    "        \n",
    "    def regularizer_step(self, p):\n",
    "        '''\n",
    "        L2 regularization: adds the square of the weights to the loss\n",
    "        -> tries to minimize the abs val of the weights\n",
    "        small weights = more generalization\n",
    "        # IMPO! also this update uses lr!\n",
    "        ''' \n",
    "        if self.weight_decay:\n",
    "            '''\n",
    "            p' = p - wd*lr*p\n",
    "            p' = p * (1-wd*lr)\n",
    "            '''\n",
    "            p *= 1 - (self.lr * self.weight_decay)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            # if you use .data is just a using with .no_grad() \n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828e3c5",
   "metadata": {},
   "source": [
    "Consider the difference between weight decay and L2 regularization:\n",
    "    \n",
    "    weights -= lr*wd*weights\n",
    "    \n",
    "vs\n",
    "\n",
    "    weights += wd*weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528cf88",
   "metadata": {},
   "source": [
    "L2 regularization penalizes large weights to prevent overfitting. \n",
    "\n",
    "2 approaches (the first was in my mind and wrong):\n",
    "\n",
    "> params -= wd * lr * params (Commonly used in some implementations):\n",
    "\n",
    "Adjusts the weights (params) by subtracting a fraction (wd * lr) of the weights themselves. It scales the amount of wd by the lr -> the larger the lr or wd, the larger the adjustment to the weights. This combined effect helps to regulate the magnitude of the weights during optimization/more effective regularization.\n",
    "\n",
    "> params -= wd * params (Alternative formulation without considering the learning rate):\n",
    "\n",
    "Without considering the learning rate, it directly penalizes the weights by subtracting a fraction of the weights themselves. However, it doesn't account for the scale at which these adjustments are made based on the learning rate.\n",
    "\n",
    "It allows for finer control over how much the weights are penalized in each update step based on the learning rate.\n",
    "\n",
    "By including the learning rate in the weight decay calculation, it provides a mechanism to scale the regularization effect relative to the step size taken during gradient descent. This combined formulation helps prevent the model from overfitting by appropriately penalizing large weights while ensuring a stable and effective optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f44cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=0.4, cbs=cbs, opt_func=SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f63a4",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "Momentum should be in the optimizer, not as we did before \n",
    "It allows you to follow the average of the directions in sgds steps in the loss func surface. The higher the mom the slower it reacts to changes of direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae26896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(SGD):\n",
    "    '''\n",
    "    Stores moving avg of the gradient and \n",
    "    Idea: take only a small fraction of the current batch gradient\n",
    "    keep going toward what the grad avg has stored over time \n",
    "    it smooths out the trajectory instead of being bumpy at each batch\n",
    "    it smoothly follows the general trend.\n",
    "    Too small = too bumpy\n",
    "    Too high = not reactive to changes, late response to surface changes\n",
    "        acts wrt info of many batches ago\n",
    "    The right value leads to a smoother, faster, stabler convergence\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, params, lr, wd=0., mom=0.9):\n",
    "        super().__init__(params, lr, wd)\n",
    "        self.mom = mom\n",
    "    \n",
    "    def one_step(self): # customizes torch dtypes cool\n",
    "        if not hasattr(p, \"grad_avg\"): # creates a new dmember for tensor class\n",
    "            p.grad_avg = torch.zeros_like(p.grad)\n",
    "        p.grad_avg = p.grad_avg*self.mom + p.grad * (1 - self.mom)\n",
    "        # what was the trend + this batch grad\n",
    "        p -= self.lr * p.grad_avg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44940708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=1.5, cbs=cbs, opt_func=Momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04681b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "astats.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294855",
   "metadata": {},
   "source": [
    "# - huge bump in lr cuz momentum allows us to follow in a smoother way the loss surface\n",
    "\n",
    "# - you want a small batch size cuz more opportunity to update \n",
    "\n",
    "# - momentum can be too aggressive for complex architectures, rmsprop better for complex architectures (this means also Adam is bad for complex architectures since also Adam uses momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbce9d3",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af712eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(SGD):\n",
    "    '''\n",
    "    Same idea of momentum but with the addition as:\n",
    "    if thre grad has been varying a log -> uncertainity over direction\n",
    "    -> we should do small steps/updates\n",
    "    -> if low variation -> confident about direction -> go faster\n",
    "    \n",
    "    p.grad**2 -> measure of spread of the gradient for each weight\n",
    "    -> its moving avg thus defines the volatility of a a single param grad\n",
    "    -> taking its .sqrt() we \"bring back variance to data scale\" \n",
    "        (grad scale in this case)\n",
    "    -> step = (grad of current batch) / (grad volatility) \n",
    "        -> if volatility high : step = small -> move slower\n",
    "        -> if volatility low : step = high -> move faster\n",
    "    '''\n",
    "    def __init__(self, params, lr, wd=0., mom=0.99, eps=1e-5):\n",
    "        super().__init__(params, lr, wd)\n",
    "        self.mom, self.eps = mom, eps\n",
    "        \n",
    "    def opt_step(self, p):\n",
    "        if not hasattr(p, \"square_avg\"):            \n",
    "            p.square_avg = p.grad**2 # non 0 init cuz this goes to denominator \n",
    "                # and it would lead to first updates very large\n",
    "            \n",
    "        p.square_avg = p.square_avg*self.mom + p.grad**2 * (1 - self.mom)\n",
    "        # divide the grad by the ammount of variation \n",
    "        p -= self.lr * p.grad / (p.square_avg.sqrt() + self.eps)\n",
    "        # the denominator is possibly a small number \n",
    "        # -> we need to decrease lr cuz \n",
    "        # p.grad / (p.square_avg.sqrt() + self.eps) is possibly very large\n",
    "        # and if lr too big we would be doing too large steps -> divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "astats.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eca3fa",
   "metadata": {},
   "source": [
    "# ADAM\n",
    "RMSProp with momentum combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77567554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(SGD):\n",
    "    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n",
    "        '''\n",
    "        beta1 = momentum\n",
    "        beta2 = momentum of squared grad (momentum from RMSProp)\n",
    "        '''\n",
    "        super().__init__(params, lr, wd)\n",
    "        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n",
    "\n",
    "    def opt_step(self, p):\n",
    "        if not hasattr(p, 'avg'): \n",
    "            p.avg = torch.zeros_like(p.grad.data)\n",
    "        if not hasattr(p, 'sqr_avg'): \n",
    "            p.sqr_avg = torch.zeros_like(p.grad.data)\n",
    "        \n",
    "        # apply momentum to g.avg\n",
    "        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n",
    "        unbias_avg = p.avg / (1 - (self.beta1**(self.batch_number+1)))\n",
    "        \n",
    "        # apply momentum to g.squared \n",
    "        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n",
    "        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.batch_number+1)))\n",
    "        \n",
    "        # normalize wrt grad variance\n",
    "        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)\n",
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "astats.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da767761",
   "metadata": {},
   "source": [
    "# Let's look at the content of a whole pytorch module: lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62815cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler. # + hit tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd37885",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(torch.optim.lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all things in torch.optim.lr_scheduler: valid attributes and methods belonging to an object\n",
    "' '.join(o for o in dir(torch.optim.lr_scheduler) if o[0].isupper() and o[1].islower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47229c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"./schedulers.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36de47",
   "metadata": {},
   "source": [
    "These schedulers work with pytorch optimizers, so we have to use those since their API is a little different from the optimizers that we implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cd497",
   "metadata": {},
   "source": [
    "# Pytorch optimizers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23959c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer by instanciating the learner using pyt.optim.sgd\n",
    "set_seed(2)\n",
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=[SingleBatchCB()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = learn.opt # fit model for 1 batch s.t. just to get its opt \n",
    "' '.join(o for o in dir(optimizer) if o[0]!='_') # amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [attr for attr in dir(optimizer) if not attr.startswith('__')]\n",
    "\n",
    "# Differentiating between methods and attributes\n",
    "methods = [attr for attr in attributes if callable(getattr(optimizer, attr)) and not attr.startswith('_')]\n",
    "attributes = [attr for attr in attributes if not callable(getattr(optimizer, attr)) and not attr.startswith('_')]\n",
    "\n",
    "print(\"optimizer Attributes:\", attributes)\n",
    "print(\"\\n\")\n",
    "print(\"optimizer Methods:\", methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_obj_API(obj):\n",
    "    attributes = [attr for attr in dir(obj) if not attr.startswith('__')]\n",
    "\n",
    "    # Differentiating between methods and attributes\n",
    "    methods = [attr for attr in attributes if callable(getattr(obj, attr)) and not attr.startswith('_')]\n",
    "    attributes = [attr for attr in attributes if not callable(getattr(obj, attr)) and not attr.startswith('_')]\n",
    "    \n",
    "    print(f\"{type(obj).__name__} Attributes:\", attributes)\n",
    "    print(\"\\n\")\n",
    "    print(f\"{type(obj).__name__} Methods:\", methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b610c8e",
   "metadata": {},
   "source": [
    "It contains \"parameters groups\", in our case it contains only 1 group that is composed by the all params of our model.\n",
    "Let's see the params groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = next(iter(learn.model.parameters())) # get param\n",
    "state = optimizer.state[param] # get state of the param \n",
    "# the state of the param is contained in a dict k:param_vect, v:state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612119ef",
   "metadata": {},
   "source": [
    "Now let's see this weird thing: a dict with as keys -> parameter tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "state # sgd momentum buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66766b20",
   "metadata": {},
   "source": [
    "State is a dictionary that stores info related to a tensor eg here it is shown the storage for moving average used for momentum. It's just as class data member but pyt in this case works with this dict approach.\n",
    "\n",
    "Optimizers handle parameters as parameter groups cuz u can change lr of particular groups ad-hoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a83d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of param groups:\", len(optimizer.param_groups))\n",
    "pg = optimizer.param_groups[0] # the retrieved obj is a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41dccb",
   "metadata": {},
   "source": [
    "we have only 1 param group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pg) #<- same as keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b85d33",
   "metadata": {},
   "source": [
    "# Schedulers\n",
    "We have already implemented a scheduler, lecture 8a so now we load pytorch ones and test them. Schedulers are able to change lr of an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ec8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLr(lr_start, steps, i):\n",
    "    return lr_start/2 * (1 + math.cos(i*math.pi/steps))\n",
    "\n",
    "\n",
    "steps = 100\n",
    "init_lr = 0.2\n",
    "list_lrs = [init_lr]\n",
    "step = partial(getLr, init_lr, steps)\n",
    "\n",
    "for i in range(steps):\n",
    "    list_lrs.append(step(i))\n",
    "    \n",
    "plt.plot(list_lrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosAnnealerScheduler():\n",
    "    \n",
    "    def __init__(self, optimizer, max_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.max_steps = max_steps\n",
    "        self.step_couter = 0\n",
    "    \n",
    "    def \n",
    "    \n",
    "    \n",
    "    def step():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5185bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "sched = CosineAnnealingLR(optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64092cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched.base_lrs # got from the optimizer\n",
    "#starting lr, list cuz different for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_obj_API(sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72673d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize the annealing of the lr\n",
    "def sched_lrs(sched, steps):\n",
    "    lrs = [sched.get_last_lr()]\n",
    "    for i in range(steps):\n",
    "        sched.optimizer.step()\n",
    "        sched.step()\n",
    "        lrs.append(sched.get_last_lr())\n",
    "    print(\"last 5 lrs: \", lrs[-5:])\n",
    "    plt.plot(lrs)\n",
    "    \n",
    "sched_lrs(sched, 110) # goes up after 100 steps cuz cosine curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f415fba",
   "metadata": {},
   "source": [
    "CosineAnnealingLR is a scheduling technique that starts with a very large learning rate and then aggressively decreases it to a value near 0 before increasing the learning rate again.\n",
    "\n",
    "Each time the “restart” occurs, we take the good weights from the previous “cycle” as the starting point. Thus, with each restart, the algorithm approaches the minimal loss closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49beac",
   "metadata": {},
   "source": [
    "# Scheduler CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSchedCB(Callback):\n",
    "    '''\n",
    "    scheduler: partial with ctor of the desired scheduler with T_max\n",
    "    \n",
    "    eg: BaseSchedCB(partial(CosineAnnealingLR, T_max=val))\n",
    "    \n",
    "    the scheduler is an handler of the optimizer\n",
    "    the optimizer is agnostic of the scheduler\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        # before fit sets gets learner optimizers and uses it to set up scheduler\n",
    "        self.scheduler_optimizer = self.scheduler(learn.opt)\n",
    "    \n",
    "    def step(self, learn):\n",
    "        if learn.training: \n",
    "            self.scheduler_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSchedCB(BaseSchedCB):\n",
    "    def after_batch(self, learn): self.step(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfd43f",
   "metadata": {},
   "source": [
    "Since BatchSchedCB and EpochSchedCB  do not have their own \\_\\_init_\\_ they implicitly call the \\_\\_init_\\_() of their superclass forwarding the param (scheduler) from instanciation of derived to ctor call of base class. \\\n",
    "On the other hand if you provide your own \\_\\_init_\\_() method you __MUST__ call super().\\_\\_init_\\_() with the right args."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff5fe7",
   "metadata": {},
   "source": [
    "To see what the scheduler is doing we need access to the inside of the learner. \n",
    "In particular we need eg to record something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "def f(**a):\n",
    "    print(a) # outs dict\n",
    "\n",
    "    t = list(a) # outs keys as list\n",
    "    print(t)\n",
    "    \n",
    "    for k, v in a.items():\n",
    "        print(k, v)\n",
    "        \n",
    "f(asd=\"asd\", foo=\"foo\", lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85924d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderCB(Callback):\n",
    "    '''\n",
    "    it takes as input a  keyworded list of args where each value is a func \n",
    "    that \n",
    "    \n",
    "    keyward = thing that we want to record\n",
    "    func = function to grab the keyward\n",
    "    '''\n",
    "    def __init__(self, **d): # d is a keyworded list of args\n",
    "        self.d = d\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        self.recs = {k:[] for k in self.d}\n",
    "        # here we record only stuff related to the first param group\n",
    "        self.param_group = learn.opt.param_groups[0] \n",
    "        \n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training:\n",
    "            return\n",
    "        \n",
    "        for k, v in self.d.items():\n",
    "            self.recs[k].append(v(self))\n",
    "            \n",
    "    def plot(self):\n",
    "        for k, v in self.recs.items():\n",
    "            plt.plot(v, label=k)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3949800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lr(cb):\n",
    "    return cb.param_group[\"lr\"] # cb is the instance of RecorderCB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff7653",
   "metadata": {},
   "source": [
    "We need to tell the scheduler the max ammount of opt.step() we are going to perform. So we take n_epochs * number_of_mini_batches_in_train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "tmax = n_epochs * len(dls.train)\n",
    "print(tmax)\n",
    "scheduler = partial(CosineAnnealingLR, T_max=tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12344cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)\n",
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "\n",
    "rec = RecorderCB(lr=_lr)\n",
    "batchSched = BatchSchedCB(scheduler)\n",
    "\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, batchSched, rec]\n",
    "\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=optim.Adam)\n",
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eaf2e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747180fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = learn.opt.param_groups[0]\n",
    "pg['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c629d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSched.scheduler_optimizer.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSchedCB(BaseSchedCB):\n",
    "    def __init__(self, scheduler, print_lr=False):\n",
    "        super().__init__(scheduler)\n",
    "        self.print_lr=print_lr\n",
    "        \n",
    "    def after_epoch(self, learn): \n",
    "        self.step(learn)\n",
    "        if self.print_lr:\n",
    "            print(self.scheduler_optimizer.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df18cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "scheduler = partial(CosineAnnealingLR, T_max=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)\n",
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "\n",
    "rec = RecorderCB(lr=_lr)\n",
    "epochSched = EpochSchedCB(scheduler, True)\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, epochSched, rec]\n",
    "\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=optim.Adam)\n",
    "learn.fit(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochSched.scheduler_optimizer.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252bd6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pg = learn.opt.param_groups[0]\n",
    "pg['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e69191",
   "metadata": {},
   "source": [
    "## CosineAnnealingLR visualization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab887b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)\n",
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=[SingleBatchCB()])\n",
    "learn.fit(1)\n",
    "optimizer = learn.opt # fit model for 1 batch s.t. just to get its opt \n",
    "sched = CosineAnnealingLR(optimizer, 3)\n",
    "sched_lrs(sched, 3) \n",
    "sched.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101001e",
   "metadata": {},
   "source": [
    "CosineAnnealingLR takes a T_max. This T_max is the ammount of steps that the scheduler knows that have to be done. \n",
    "Therefore CosineAnnealingLR also has an implicit \"range\" (even if it is implemented in a recursive manner): from lr_start to lr=0 it have to occurr T_max steps -> once the training is over the scheduler does \"last step\" and sets lr=0.0 cuz annealer is exausted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b690d09",
   "metadata": {},
   "source": [
    "# 1-cycle training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beta1(cb):  # adams momentum\n",
    "    return cb.param_group['betas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = RecorderCB(lr=_lr, mom=_beta1) \n",
    "# 2 things are being tracked -> 2 things will be plotted at rec.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dce224",
   "metadata": {},
   "source": [
    "## Idea behind the OneCycleLR:\n",
    "lr starts low that raises and goes back down\n",
    "lr must start low cuz in the beginning we don't have a perfectly initialized model, so low learning rate allows us to start training without \"derailing\" off track.\n",
    "While the lr is low, the core push will come from momentum -> if the weights keep moving in the same dir, even if lr is low, momentum will push us toward the right \"underlying\" direction.\n",
    "Then when we get to the \"righ part\" of the area of the weight space, we can use high lr, but when we have high lr we must decrease/have very low momentum cuz otherwise we would jump around too much.\n",
    "Then when we are close to convergence we need to reduce the lr for fine tuning.\n",
    "\n",
    "\n",
    "while the momentum starts high, then drops to then raise again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "lr = 6e-2\n",
    "n_epochs = 5\n",
    "tmax = n_epochs * len(dls.train)\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "scheduler = partial(OneCycleLR, total_steps=tmax, max_lr=lr)\n",
    "\n",
    "model = get_model(general_leaky_relu, norm=nn.BatchNorm2d).apply(f_init_weights)\n",
    "\n",
    "epochSched = BatchSchedCB(scheduler)\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, epochSched, rec]\n",
    "\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs, opt_func=optim.Adam)\n",
    "learn.fit(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75343995",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00af0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 cool things from https://github.com/danielegrattarola/GINR/blob/master/src/models/graph_inr.py\n",
    "optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, patience=self.lr_patience, verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
