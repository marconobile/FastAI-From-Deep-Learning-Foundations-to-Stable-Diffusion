{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64654e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ff8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: \n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15ba64",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9618a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50\n",
    "n_in = 784\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a93f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7622181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(784, 50, 10) # now 10 cuz we have 10 labels\n",
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b021f510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86448f31",
   "metadata": {},
   "source": [
    "# Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5858b97",
   "metadata": {},
   "source": [
    "Given that we have 10 categories\n",
    "We compare a 10 els vector of probabilities -> argmax -> vs the integer target\n",
    "\n",
    "Let's compute softmax: (activation func of last layer to send lin layer out into probability space)\n",
    "$$ \\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j} }}$$\n",
    "\n",
    "but we actually want log of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "383a7d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.05,  0.03,  0.22,  0.02,  0.00, -0.09, -0.04, -0.12, -0.15,  0.20], grad_fn=<CloneBackward0>),\n",
       " torch.Size([10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = preds[0].clone()\n",
    "test, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb407691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.08, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56067ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((test.exp()/test.exp().sum()).log()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60ccc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): \n",
    "    # return (x.exp()/x.exp().sum(-1, keepdim=True)).log()\n",
    "    return (x.exp()/x.exp().sum(-1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8915751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a890785",
   "metadata": {},
   "source": [
    "Recalling the properties of logs, we can simply the computation as: (s.t. avoid division/multiplication cuz they might lead to \n",
    "numerical instability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55614f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b31a7",
   "metadata": {},
   "source": [
    "This definition of log softmax might incurr in overflow: in the denominator sum of e^xi if any xi is a large number-> e^xi will be much larger -> overflow. Thus we apply the LogSumExp trick:\n",
    "consider:\n",
    "\n",
    "$$ a=max(x_{j}) $$\n",
    "then:\n",
    "$$ log \\left(\\sum_{j=1}^{n} e^{x_{j}}\\right) = log \\left(e^{a} \\sum_{j=1}^{n} e^{x_{j}-a}\\right) = a + log \\left(\\sum_{j=1}^{n}{e^{x_{j}-a}} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "685278ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()    \n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1,keepdim=True) # x.logsumexp pyt version of logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203b54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(logsumexp(preds), preds.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d44c0",
   "metadata": {},
   "source": [
    "since we have a multilabel plroblem (not binar label) we can use as softmax:\n",
    "    $$ -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}) $$\n",
    "- M - number of classes\n",
    "- y binary indicator (0 or 1) if class label c is the correct classification for observation o\n",
    "- p predicted probability observation o is of class c\n",
    "\n",
    "Since y is one hot encoded, the loss for an obs i and supposing that the correct class is the third (idx = 3), is:\n",
    "$$ L_{i} = -\\log(\\hat{y}_{i, 3}) $$\n",
    "that is called negative cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d237795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot here y = -log(x) with 0<=x<=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009f111",
   "metadata": {},
   "source": [
    "# Indexing \n",
    "To avoid making all targets 1-hot-encoded, we can use indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd6e3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]\n",
    "# these are the correct classes for the first 3 obs in x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "889d4ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.09, grad_fn=<SelectBackward0>),\n",
       " tensor(-0.07, grad_fn=<SelectBackward0>),\n",
       " tensor(0.15, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability extimated for the correct class are:\n",
    "preds[0,5],preds[1,0], preds[2,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7b0f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.09, -0.07,  0.15], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do the same of above:\n",
    "preds[[0,1,2], y_train[:3]]\n",
    "# [0,1,2] list of idxs to select rows from preds\n",
    "# y_train[:3] list of idxs to select from each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f98c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative cross entropy loss\n",
    "def nll(inp, targets):\n",
    "    return -inp[range(targets.shape[0]), targets].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0017e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(preds) # take raw preds and pass em thru log_soft_max\n",
    "loss = nll(sm_pred, y_train) # compute loss\n",
    "\n",
    "# nll in pytorch is called F.nll_loss\n",
    "test_close(F.nll_loss(F.log_softmax(preds,-1), y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f1bb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in pytorch F.log_softmax and F.nll_loss are combined in one optimized func\n",
    "# called F.cross_entropy\n",
    "test_close(F.cross_entropy(preds, y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7a38b",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22297bf1",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cebbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b234682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.05,  0.03,  0.22,  0.02,  0.00, -0.09, -0.04, -0.12, -0.15,  0.20], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=50                  # batch size\n",
    "\n",
    "xb = x_train[:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a347248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,\n",
       "        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:bs]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4d112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.28, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a7ac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 4, 2, 2, 9, 2, 2, 1, 9, 9, 9, 2, 2, 1, 2, 2, 2, 9, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 1, 9, 1, 2, 2, 2, 9, 2, 9, 2,\n",
       "        9, 9, 2, 9, 2, 9, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1) # for predictions use argmax over each pred row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "568d2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, y):\n",
    "    return (out.argmax(dim=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e13248d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c417292",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f648847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.2825334072113037, acc 0.1599999964237213\n",
      "loss 0.21871758997440338, acc 0.9399999976158142\n",
      "loss 0.13389278948307037, acc 0.9599999785423279\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs)) # list of idxs of bach\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        if i == 0: # at the beginning of each epoch\n",
    "            print(f'loss {loss}, acc {accuracy(preds, yb)}')\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'): # it is a layer that has weights\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32287bba",
   "metadata": {},
   "source": [
    "Let's now start refactoring -> doing the same things with less code\n",
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d34e6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how the nn.Module() class works\n",
    "# we don't normally use it like this, so let's just look at how it works\n",
    "module = nn.Module()\n",
    "module.foo = nn.Linear(3,4) # create an attribute of the module\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ddec58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can grab all the children of the module, that acts as root\n",
    "list(module.named_children()) # list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f567dab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.24, -0.11,  0.46],\n",
       "         [-0.17, -0.46, -0.07],\n",
       "         [-0.29,  0.31,  0.08],\n",
       "         [-0.24,  0.13, -0.17]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.01,  0.51, -0.11, -0.47], requires_grad=True)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all params of the module\n",
    "list(module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86e73d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually we inherit nn.module and then we build our own network on top of it\n",
    "# nn.module allows automatic param tracking as we've seen above\n",
    "class MLP(nn.Module):\n",
    "    # a custom pyt module, it knows its attribute and its parameters\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.asd = \"hello\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.relu(self.l1(x)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ac1a9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(784, 50, 10)\n",
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aff1c707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # prints all data members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347228df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, pytObj in model.named_children():\n",
    "    print(f\"{name}: {pytObj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d120497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# a custom pyt module, it knows its parameters\n",
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da2e182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is useful cuz we can change the train loop as following\n",
    "\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n, bs):\n",
    "            s = slice(i, min(n, i+bs)) # list of idxs of bach\n",
    "            xb, yb = x_train[s], y_train[s]\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            if i == 0: # at the beginning of each epoch\n",
    "                print(f'loss {loss}, acc {accuracy(preds, yb)}')\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    param -= param.grad * lr\n",
    "                model.zero_grad()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "922633f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3002638816833496, acc 0.14000000059604645\n",
      "loss 0.1547200083732605, acc 0.9200000166893005\n",
      "loss 0.164123997092247, acc 0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "921d7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But how can a nn.Module keep track of the params it contains?\n",
    "# __setattr__(self,args)\n",
    "\n",
    "# let's create nn.Module from scratch\n",
    "class MyModule(object): # object is always omitted but we inherit from it\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {} # private container dict\n",
    "        self.l1 = nn.Linear(n_in, nh) # this actually calls __setattr__\n",
    "        self.l2 = nn.Linear(nh, n_out) # this actually calls __setattr__\n",
    "    \n",
    "    # __setattr__ to create a datamember at runtime for this obj:\n",
    "    # this.foo = x will create a dmember self.foo = x\n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith(\"_\"): # if the datamember is not private\n",
    "            self._modules[k] = v\n",
    "        super().__setattr__(k,v) # calls the method of python Class code that\n",
    "        # actually does the creation of the dmember\n",
    "        \n",
    "    # defines how to print this\n",
    "    def __repr__(self):\n",
    "        return f'{self._modules}'\n",
    "\n",
    "    def parameters(self):\n",
    "        for l in self._modules.values(): # for each layer\n",
    "            yield from l.parameters() # creates generator from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "223566cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myMod = MyModule(784, 50, 10)\n",
    "myMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f486cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in myMod.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c79aac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we can use pytorch nn.Module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042716b",
   "metadata": {},
   "source": [
    "# Registering Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23b2b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bab08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the model above we had as data member:\n",
    "# self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "966c95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model now takes as input in ctor a list of layers\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__() # always call this!\n",
    "        self.layers = layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.add_module(f'layer_{i}', l) # base class method\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # reduce/reduction: \n",
    "        # reduce(func, sequence, initial_val) -> val\n",
    "        # start with x, then apply f(x,l) where x is init val and l is first\n",
    "        # el of the list, then l is iterated over sequence\n",
    "        # while val is the output of the prev f(x,l)\n",
    "        \n",
    "        return reduce(lambda val, layer: layer(val), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "053db642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (layer_1): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myMod = Model(layers)\n",
    "myMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7d18b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pytorch implementation of the above:\n",
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers) # now this handles the loop above\n",
    "        # it registres Layers automatically for us\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a77b19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myMod = SequentialModel(layers) # nn.Squential\n",
    "myMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d938f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.319960594177246, acc 0.07999999821186066\n",
      "loss 0.1238829717040062, acc 0.9599999785423279\n",
      "loss 0.06840486824512482, acc 0.9599999785423279\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out))\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe100f1d",
   "metadata": {},
   "source": [
    "# Optim\n",
    "Optimizer of params is crucial, let's implement it on our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cf25b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr = 0.5):\n",
    "        self.params = list(params) # impo to take them as list! (no generator)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_() # .data is a way to avoid with torch.no_grad():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fefcd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out))\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c71b50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(loss, preds, yb):\n",
    "    print(f'loss {loss}, acc {accuracy(preds, yb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13a311fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.10364146530628204, acc 0.9800000190734863\n",
      "loss 0.0641181617975235, acc 1.0\n",
      "loss 0.05478580296039581, acc 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f4c77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we can use pyt optimizers!\n",
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out))\n",
    "    return model, optim.SGD(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72a9c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcecbe6",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n",
    "We can improve our training loop code by creating an abstraction to handle\n",
    "batching, xb, yb.\n",
    "\n",
    "Goal: \n",
    "\n",
    "x_batch, y_batch = trainDataset[batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b13de115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i): # overloader for [] operator, supports slicing\n",
    "        # this method could have a lot of logic, usually is parallelized\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e955fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72e42da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = train_ds[0:5]; xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ac63b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c1ed3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.13820990920066833, acc 0.9399999976158142\n",
      "loss 0.096625916659832, acc 0.9599999785423279\n",
      "loss 0.07814852893352509, acc 0.9599999785423279\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = train_ds[s] # could also use directly [i:min(n, i+bs)]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991b5aa",
   "metadata": {},
   "source": [
    "Now we want to reduce the train loop even further by creating an iterator (by implementing the \\_\\_iter\\_\\_() method) that returns an iterable of batches of (x_batch,y_batch), i.e. a data loader\n",
    "Goal:\n",
    "\n",
    "for x_batch, y_batch in dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b947fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batchsize):\n",
    "        self.dataset, self.batchsize = dataset, batchsize\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize):\n",
    "            yield self.dataset[i:i+self.batchsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "132af978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we can\n",
    "# 1) create dataset\n",
    "# 2) use it to create a dataloader\n",
    "# 3) iterate over batches of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a77e979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "trainDataLoader, validDataLoader = DataLoader(train_ds, batchSize), DataLoader(valid_ds, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ec02030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.18502801656723022, acc 0.9375\n",
      "loss 0.0874142050743103, acc 0.9375\n",
      "loss 0.03932715207338333, acc 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb, yb in trainDataLoader:\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2ddc7",
   "metadata": {},
   "source": [
    "So now we have the core backbone of a training loop; thus we can start adding features!\n",
    "# Random Sampling\n",
    "Training set must be shuffled at each epoch (nb: validation shouldnt).\n",
    "To do so we are going to create a class \"Sampler\" that will shuffle train data idxs (that have to be applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8677647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sampler():\n",
    "    \n",
    "    # returns an iterator of shuffled idxs for input dataset\n",
    "    \n",
    "    def __init__(self, dataset, shuffle=False):\n",
    "        self.n = len(dataset)\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __iter__(self): # this must return an iterator object\n",
    "        res = list(range(self.n))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(res)\n",
    "        return iter(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb1ab782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "s = Sampler(train_ds) # not shuffled\n",
    "list(islice(s,5)) \n",
    "# islice make an iterator that returns selected elements from the iterable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a3566ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8583, 40714, 37864, 19868, 36819]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(train_ds, shuffle=True)\n",
    "list(islice(s,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb43a01",
   "metadata": {},
   "source": [
    "So now we can create a BatchSampler: a BatchSampler will return batchSized\n",
    "randomized idxs that are going to be used to grab data from DataLoader.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68059f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc\n",
    "\n",
    "class BatchSampler():\n",
    "    def __init__(self, sampler, batchSize, drop_last=False):\n",
    "        self.sampler = sampler\n",
    "        self.batchSize = batchSize \n",
    "        self.drop_last = drop_last\n",
    "    \n",
    "    # look at lecture2 for chunk\n",
    "    # it splits a list in a list of lists of desired size\n",
    "    def __iter__(self):\n",
    "        # returns list of random idxs for given batch\n",
    "        yield from fc.chunked(iter(self.sampler), self.batchSize, self.drop_last) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60beb7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25551, 47757, 13043, 36752, 44096],\n",
       " [504, 37364, 42213, 47094, 29046],\n",
       " [2645, 45715, 7041, 13751, 17650],\n",
       " [6643, 39060, 6680, 26018, 5960],\n",
       " [3130, 2236, 35033, 8108, 28336]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchesIdxs = BatchSampler(s, 5)\n",
    "list(islice(batchesIdxs,5)) \n",
    "\n",
    "# so now we have a way to iterate thru all dataset grabbing\n",
    "# batches of random idxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "754b64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b): # b = [(xi,yi), (xj,yj), ...]\n",
    "    xs, ys = zip(*b) # creates 2 lists with all xis and all yis\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e7bdcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[5][0].shape, train_ds[5][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f4ac22af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([2, 1]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate([train_ds[5], train_ds[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d770a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can now rewrite our DataLoader with this new functionality\n",
    "class DataLoader():\n",
    "    \n",
    "    # using dataset\n",
    "    # and batchSampler\n",
    "    # returns batches of data\n",
    "    \n",
    "    def __init__(self, dataset, batchSampler, collate_fn = collate):\n",
    "        self.dataset = dataset\n",
    "        self.batchSampler = batchSampler\n",
    "        self.collate_fn = collate_fn\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield from (self.collate_fn(self.dataset[i] for i in batch) for batch in self.batchSampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "990ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyt DataLoader works as:\n",
    "# dataset # its idxing returns (xi,yi)\n",
    "# samplers # shuffles idxs\n",
    "# batch sampler # randomizes idxs\n",
    "# collate \n",
    "# dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a1f9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "train_sampler = BatchSampler(Sampler(train_ds), batchSize)\n",
    "train_dataLoader = DataLoader(train_ds, train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5047cf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dataLoader))\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.imshow(xb[0].view(28,28))\n",
    "xb.shape # batchsize rows, each row is an obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333879a",
   "metadata": {},
   "source": [
    "# MultiProcessing DataLoader\n",
    "Dataloaders in pyt do exactly what we have seen above, but the process\n",
    "to pre-process data to be returned when indexing into a dataset is done on \n",
    "different processors. The standard python api for multiprocessing is: _multiprocessing_, so we might use it, but instead we use _torch.multiprocessing_ because standard py lib does not work well with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d200901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3502c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing in dataset and calling its __getitem__ is equivalent\n",
    "\n",
    "x1, y1 = train_ds[[3,6,9]]\n",
    "x2, y2 = train_ds.__getitem__([3,6,9])\n",
    "\n",
    "assert torch.all(x1 == x2) == tensor(True)\n",
    "assert torch.all(y1 == y2) == tensor(True)\n",
    "\n",
    "x1, y1 = train_ds[3:9]\n",
    "x2, y2 = train_ds.__getitem__(range(3, 9))\n",
    "\n",
    "assert torch.all(x1 == x2) == tensor(True)\n",
    "assert torch.all(y1 == y2) == tensor(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "744dbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so to parellalize we can call map with the __getitem__ func:\n",
    "# map(func, sequence)\n",
    "# map takes a seq and applies a func to each el of that sequence \n",
    "# (with that el as input of the func that is being mapped)\n",
    "# returns a seq of the same length of the input seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "088fecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# thus for example to break a batch of 4 els \n",
    "# with idxs 3, 6, 8, 1\n",
    "\n",
    "for o in map(train_ds.__getitem__, ([3, 6], [8, 1])):\n",
    "    print(o)\n",
    "# this grabs the first el of input seq, whihc is the list [3, 6]\n",
    "# calls and f:train_ds.__getitem__ with that input\n",
    "# we do this because multiprocessing has a class that is able to instanciate\n",
    "# serveral threads (i.e. a pool of threads -> where each thread is a \"worker\")\n",
    "# thus we can run map where each func is applied over the seq in a separate thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "48e04948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batchSampler, n_workers=1, collate_fn=collate):\n",
    "        self.dataset = dataset\n",
    "        self.batchSampler = batchSampler\n",
    "        self.n_workers = n_workers\n",
    "        self.collate_fn = collate_fn\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # using n threads execute map\n",
    "        with mp.Pool(self.n_workers) as ex:\n",
    "            yield from ex.map(self.dataset.__getitem__, iter(self.batchSampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2702aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this cell hangs in jupyter\n",
    "# train_dl = DataLoader(train_ds, batchSampler= train_sampler, n_workers=2)\n",
    "# it = iter(train_dl)\n",
    "# xb, yb = next(it)\n",
    "# xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb3ff4",
   "metadata": {},
   "source": [
    "# PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94f1af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e37916",
   "metadata": {},
   "source": [
    "RandomSampler returns randomized idxs\n",
    "\n",
    "SequentialSampler returns normal ordering of idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05042e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "\n",
    "train_sampler = BatchSampler(RandomSampler(train_ds), batch_size=bs, drop_last= False)\n",
    "valid_sampler = BatchSampler(SequentialSampler(valid_ds), batch_size=bs, drop_last= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1207fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataLoader = DataLoader(dataset=train_ds, \n",
    "                              # batch_size=bs, # in this setting bs is in sampler\n",
    "                              sampler=train_sampler, \n",
    "                              collate_fn=collate)\n",
    "\n",
    "val_dataLoader = DataLoader(valid_ds, \n",
    "                            sampler=valid_sampler, \n",
    "                            collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57af434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.12716785073280334, acc 0.9375\n",
      "loss 0.13605919480323792, acc 0.9375\n",
      "loss 0.0028977771289646626, acc 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_model(n_in, nh, n_out):\n",
    "    model = nn.Sequential(nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out))\n",
    "    return model, optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "model, opt = get_model(784, 50, 10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dataLoader:\n",
    "        xb.squeeze_() # there are some issues on shapes, keep going\n",
    "        yb.squeeze_()\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76465369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can avoid defining the BatchSampler: we can just provide bs to datalader\n",
    "# and wheter to shuffle data or not (auto creation of Sequential/Random Samplers)\n",
    "\n",
    "train_dataLoader = DataLoader(train_ds, bs, shuffle=True, drop_last=True) # num_workers = 2\n",
    "valid_dataLoader = DataLoader(valid_ds, bs, shuffle=False) # num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed933181",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c1b85cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1906106024980545, acc 0.9375\n",
      "loss 0.08068399131298065, acc 0.984375\n",
      "loss 0.014962541870772839, acc 1.0\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model(784, 50, 10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dataLoader:        \n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac060515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick to speed up things:\n",
    "\n",
    "train_sampler = BatchSampler(RandomSampler(train_ds), batch_size=bs, drop_last= False)\n",
    "valid_sampler = BatchSampler(SequentialSampler(valid_ds), batch_size=bs, drop_last= False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, sampler = train_sampler)\n",
    "valid_dl = DataLoader(valid_ds, sampler = train_sampler)\n",
    "\n",
    "# pass to dataloader the sampler directly, cuz dataset knows how to retrieve\n",
    "# set of idxs\n",
    "\n",
    "# DataSet -> grab idxs -> returns obs, returns x[i],y[i], supports slicing\n",
    "\n",
    "# Sampler -> returns list of idxs of the whole dataset (shuffled or not)\n",
    "\n",
    "# BatchSampler -> breaks in batches of idxs the idxs returned by the Sampler\n",
    "\n",
    "# DataLoader -> indexes in DataSet using batches of idxs returned by BatchSampler, collate list of DataSet[idx] together in torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# The trick is to pass the sampler directly to the data loader with a batchSize, s.t. the dataloader itself uses the idxs returned by the Sampler direectly to index in DataSet, using the batchSize to split up \n",
    "    # \tand without using/instaciating BatchSampler and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9c6ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.21624523401260376, acc 0.9375\n",
      "loss 0.12360577285289764, acc 0.96875\n",
      "loss 0.1926768571138382, acc 0.953125\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model(784, 50, 10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dataLoader:        \n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb16c8",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6be0ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc, count = 0.,0.,0\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred, yb).item()*n\n",
    "                tot_acc += accuracy(pred, yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "826d05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataLoaders(train_ds, valid_ds, bs, **kwards):        \n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwards), DataLoader(valid_ds, batch_size=bs*2, **kwards))\n",
    "# you can double bs for validation data loader cuz it wont have to do backprop and thus half mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b98d5009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.16301606733798982 0.9556\n",
      "1 0.144886371332407 0.959\n",
      "2 0.15856560630053282 0.9515\n",
      "3 0.3052870091378689 0.9228\n",
      "4 0.10596682603061199 0.9704\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_dataLoaders(train_ds, valid_ds, 64)\n",
    "model, opt = get_model(784, 50, 10)\n",
    "loss,acc= fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
