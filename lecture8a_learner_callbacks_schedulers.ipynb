{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52255616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,torch,matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from operator import attrgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from operator import attrgetter\n",
    "import matplotlib as mpl\n",
    "import torchvision.transforms.functional as TF\n",
    "from contextlib import contextmanager\n",
    "from torch import nn,tensor\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "import logging\n",
    "from fastcore.test import test_close\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b90cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = \"mps\" if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67548db9",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61438639",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): \n",
    "    b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n",
    "\n",
    "def get_model(): \n",
    "    m,nh = 28*28,50\n",
    "    return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))    \n",
    "\n",
    "class DataLoaders:\n",
    "    def __init__(self, train_data_loader, valid_data_loader):\n",
    "        self.train = train_data_loader\n",
    "        self.valid = valid_data_loader\n",
    "\n",
    "    @classmethod # static method\n",
    "    def from_datasetDict(cls, datasetDict, batch_size): #, as_tuple=True):\n",
    "        return cls (*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds)) for ds in datasetDict.values()])\n",
    "        # this return calls __init__\n",
    "    # static method with cls allows the instanciation of the class\n",
    "    # recall that DataLoader can use multiple workers\n",
    "    # dont send anything to device here cuz huge overload \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23297dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = 'image','label'\n",
    "dsd = load_dataset(\"fashion_mnist\")\n",
    "bs = 1024\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_datasetDict(tds, bs)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1ef1f",
   "metadata": {},
   "source": [
    "# Learner with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "838eaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cbs(cbs, method_name): # core function!\n",
    "    \n",
    "    '''\n",
    "        Loops ovel all cbs in input list and for each cb obj calls its .method_name() method\n",
    "    '''\n",
    "    \n",
    "    for cb in sorted(cbs, key=attrgetter('order')): \n",
    "        # attrgetter('name'), the call f(b) returns b.name; similar to __getattr__        \n",
    "        method = getattr(cb, method_name, None)\n",
    "        if method is not None:\n",
    "            method()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5b76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(): order = 0 # order of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ac1bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ad-hoc exceptions\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e29ad74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit executed, completed 1 batches\n"
     ]
    }
   ],
   "source": [
    "# example of simple callback and functioning of run_cbs()\n",
    "class CompletionCB(Callback):\n",
    "    def before_fit(self):\n",
    "        self.count = 0\n",
    "    def after_batch(self):\n",
    "        self.count +=1\n",
    "    def after_fit(self):\n",
    "        print(f\"Fit executed, completed {self.count} batches\")\n",
    "\n",
    "cbs = [CompletionCB()]\n",
    "run_cbs(cbs, \"before_fit\")\n",
    "run_cbs(cbs, \"after_batch\")\n",
    "run_cbs(cbs, \"after_fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7900e98",
   "metadata": {},
   "source": [
    "Everything is always set as self.foo such to make everything modifiable via callbacks, since callbacks have a reference to this learner\n",
    "Learner obj and Callbacks obj are very coupled but high performance cuz of direct access across each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3652734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD):\n",
    "        fc.store_attr()\n",
    "        for cb in cbs:\n",
    "            cb.learner = self # in each Callback object create a reference dmember to this learner        \n",
    "            \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback(\"before_fit\") # calls all .before_fit() methods of all cb objs stored in self.cbs\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback(\"after_fit\") \n",
    "        except CancelFitException: \n",
    "            pass # if any of the callback for \"before_fit\" and/or \"after_fit\" throws ONLY THIS PARTICULAR EXCEPTION, do nothing\n",
    "        \n",
    "    def callback(self, method_name):\n",
    "        run_cbs(self.cbs, method_name)\n",
    "            \n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback(\"before_epoch\") \n",
    "            for self.iter, self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback(\"before_batch\") \n",
    "                    self.one_batch()\n",
    "                    self.callback(\"after_batch\")                     \n",
    "                except CancelBatchException:\n",
    "                    pass    \n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException:\n",
    "            pass \n",
    "        \n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a85170b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit executed, completed 69 batches\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(model=get_model(), dls=dls, loss_func=F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919370f7",
   "metadata": {},
   "source": [
    "# Other useful callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9b4c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceCB(Callback):\n",
    "    '''\n",
    "        self.learner set up into Learner ctor\n",
    "    '''\n",
    "    def __init__(self, device=default_device): \n",
    "        self.device = device\n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.learner.model.to(self.device)\n",
    "    \n",
    "    def before_batch(self):\n",
    "        self.learner.batch = to_device(self.learner.batch, self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d162c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [DeviceCB()]\n",
    "learn = Learner(get_model(), dls, F.cross_entropy, 0.2, cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f1e3e",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77bb8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    '''\n",
    "        Base class to be extended if particular metric is desired.\n",
    "        If not extended it computes the weighted average of its input wrt batch_size \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.vals, self.ns = [], []\n",
    "        \n",
    "    def add(self, input, target=None, batch_size=1):\n",
    "        '''\n",
    "            adds (x_hats, y) for minibatch\n",
    "            target: optional    \n",
    "        '''\n",
    "        self.last = self.calc(input, target)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(batch_size)\n",
    "    \n",
    "    @property # allaws call of value without ()\n",
    "    def value(self):\n",
    "        ns = torch.tensor(self.ns)\n",
    "        return (torch.tensor(self.vals)*ns).sum()/ns.sum()\n",
    "    \n",
    "    def calc(self, inputs, targets): \n",
    "        ''' method to be overwritten in derived class '''\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015073fc",
   "metadata": {},
   "source": [
    "# Accuracy subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c83340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    def calc(self, inputs, targets):\n",
    "        return (inputs==targets).float().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd551b",
   "metadata": {},
   "source": [
    "## Now that we have implemented metrics on our own we can use pytorch ones: https://torchmetrics.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f2ec062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchmetrics\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy #(https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#multiclassaccuracy)\n",
    "from torchmetrics.aggregation import MeanMetric # (https://torchmetrics.readthedocs.io/en/stable/aggregation/mean.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a7a1b",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dac5f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7556)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = ((tensor(dsd['train'][y]).unique()).max()+1).item()\n",
    "metric = MulticlassAccuracy(n_classes)\n",
    "\n",
    "x_hat_b1 = tensor([0,1,2,0,1]) \n",
    "y_b1 = tensor([0,0,2,1,1]) \n",
    "\n",
    "x_hat_b2 = tensor([1,1,2,0,0]) \n",
    "y_b2 = tensor([0,1,2,0,0]) \n",
    "\n",
    "metric.update(x_hat_b1, y_b1)\n",
    "metric.update(x_hat_b2, y_b2)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7819f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarcoNobile\\miniconda3\\envs\\pyt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.reset()\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0510006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8824), 0.8823529411764706)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MeanMetric()\n",
    "\n",
    "loss.update(.9, weight=32)\n",
    "loss.update(.6, weight=2)\n",
    "\n",
    "loss.compute(), (.9*32 +.6*2)/(32+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e909e50",
   "metadata": {},
   "source": [
    "So let's now create a MetricsCB class to be inserted in our cbs execution list.\n",
    "This class uses pytorch metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7778aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCB(Callback):\n",
    "    '''\n",
    "    Stores all metrics to be used/computed by model. Unique access point/handler for all metrics/to compute all metrics \n",
    "    \n",
    "    You can construct as:\n",
    "        metric = MetricCB(MulticlassAccuracy(n_classes))\n",
    "    or:\n",
    "        metric = MetricCB(accuracy=MulticlassAccuracy(n_classes))\n",
    "\n",
    "    in the first case *ms will contain: MulticlassAccuracy and by taking its name stores it in metrics dict\n",
    "    in the second case **metrics dict will contain {'accuracy'= MulticlassAccuracy()}\n",
    "    '''\n",
    "    \n",
    "    # *ms = list of positional non-keyworded inputs\n",
    "    # **metrics = dict of keyworded inputs\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms:\n",
    "            metrics[type(o).__name__] = o # as explained above adds non-keyworded inputs to **metrics       \n",
    "        self.metrics = metrics # store **metrics dict\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = MeanMetric() # MeanMetric() added by default\n",
    "        \n",
    "    def _log(self, d): \n",
    "        # to override for more complex presentations\n",
    "        print(d)\n",
    "        \n",
    "    def before_fit(self): # IMPORTANT        \n",
    "        # set this MetricCB obj in learner\n",
    "        self.learner.metrics = self\n",
    "        \n",
    "    def after_batch(self):\n",
    "        x, y = self.learner.batch\n",
    "        for m in self.metrics.values():\n",
    "            m.update(self.learner.preds, y) # for each metric, add/update (x_hats, y) for minibatch \n",
    "        self.loss.update(self.learner.loss, weight=len(x))\n",
    "    \n",
    "    def after_epoch(self): # creates dict for printing/logging \n",
    "        log = {k:f'{v.compute()}' for k,v in self.all_metrics.items()} # compute all metrics \n",
    "        log['epoch'] = self.learner.epoch\n",
    "        log['train'] = self.learner.model.training\n",
    "        self._log(log)\n",
    "        \n",
    "    def before_epoch(self): # reset        \n",
    "        [o.reset() for o in self.all_metrics.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80a46095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MulticlassAccuracy': '0.5950833559036255', 'loss': '1.186214804649353', 'epoch': 0, 'train': True}\n",
      "{'MulticlassAccuracy': '0.6886999607086182', 'loss': '0.8226090669631958', 'epoch': 0, 'train': False}\n"
     ]
    }
   ],
   "source": [
    "cbs = [MetricCB(MulticlassAccuracy(n_classes))]\n",
    "learn = Learner(get_model(), dls, F.cross_entropy, 0.2, cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7dac0",
   "metadata": {},
   "source": [
    "# More flexible learner with context manager\n",
    "### Code design: exceptions as control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78f814a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCB(Callback):\n",
    "    '''cb that handles train process details'''\n",
    "    def predict(self):\n",
    "        self.learner.preds = self.learner.model(self.learner.batch[0])\n",
    "    \n",
    "    def get_loss(self):\n",
    "        self.learner.loss = self.learner.loss_func(self.learner.preds, self.learner.batch[1])\n",
    "        \n",
    "    def backward(self):\n",
    "        self.learner.loss.backward()\n",
    "        \n",
    "    def step(self):\n",
    "        self.learner.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.learner.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b84b22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD):\n",
    "        fc.store_attr()\n",
    "        for cb in cbs:\n",
    "            cb.learner = self \n",
    "            \n",
    "    @contextmanager # defines a with: statement\n",
    "    def callback_ctx(self, context_name):\n",
    "        '''\n",
    "        When the with is executed, all code before the yield in this method is executed.\n",
    "        When the yield is called, the body of the with statement is executed.\n",
    "        After the body of the with statement is executed, the code after the yield is executed.\n",
    "        '''\n",
    "        try: \n",
    "            self.callback(f'before_{context_name}') \n",
    "            yield # here it is called all code that is in the with statement\n",
    "            self.callback(f'after_{context_name}')\n",
    "        except globals()[f'Cancel{context_name.title()}Exception']: \n",
    "            # all globals live in globals() dict, here it is getting the correct exception via classname; title capitalizes \n",
    "            pass\n",
    "   \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs \n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        with self.callback_ctx(\"fit\"): # calls all .before_fit() of this learner callbacks\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            # calls all .after_fit() of this learner callbacks\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        with self.callback_ctx('epoch'):\n",
    "            for self.iter, self.batch in enumerate(self.dl):\n",
    "                with self.callback_ctx(\"batch\"): \n",
    "                    # here will be handled all training details taken from TrainCB \n",
    "                    self.predict() # these are extracted to make learner very flexible\n",
    "                    self.get_loss() # you can plug whatever logic you want here now \n",
    "                    # first is solved self.g as dmember via __getattr__ -> returns a func; then () to call returned func\n",
    "                    if self.model.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()                \n",
    "\n",
    "    def callback(self, method_name):\n",
    "        run_cbs(self.cbs, method_name)\n",
    "                        \n",
    "    def __getattr__(self, name):\n",
    "        if name in (\"predict\", \"get_loss\", \"backward\", \"step\", \"zero_grad\"):\n",
    "            return partial(self.callback, name) # returns the input func, with input \"named\" passed to input func\n",
    "        raise AttributeError(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a54b2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MulticlassAccuracy': '0.5978000164031982', 'loss': '1.168588638305664', 'epoch': 0, 'train': True}\n",
      "{'MulticlassAccuracy': '0.6824000477790833', 'loss': '0.8261908292770386', 'epoch': 0, 'train': False}\n"
     ]
    }
   ],
   "source": [
    "cbs = [TrainCB(), MetricCB(MulticlassAccuracy(n_classes))]\n",
    "learn = Learner(get_model(), dls, F.cross_entropy, 0.2, cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df38b2",
   "metadata": {},
   "source": [
    "# MomentumLearner or MomentumCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same is by subclassing the learner directly and/or implementing yourself directly \n",
    "# the methods required to train the learner\n",
    "\n",
    "# IDEA: instead of zeroing gradients we keep them \"alive\" but we shrink them by a factor momentum < 1\n",
    "# BN: pytorch autograd ONLY ADDS to gradients\n",
    "class MomentumLearner(Learner):\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD, momentum=.85):\n",
    "        self.momentum=momentum\n",
    "        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n",
    "\n",
    "    def predict(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "    \n",
    "    def get_loss(self):\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        \n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters():\n",
    "                p.grad *= self.momentum\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332929c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [TrainCB(), MetricCB(MulticlassAccuracy(n_classes))]\n",
    "learn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=cbs, momentum=.85)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac6e15",
   "metadata": {},
   "source": [
    "# LRFinderCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, lr_multiplier=1.3):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.lrs, self.losses = [], []\n",
    "        self.min = math.inf\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if not self.learner.model.training: \n",
    "            raise CancelEpochExceptionLoss()\n",
    "        \n",
    "        self.lrs.append(self.learner.opt.param_groups[0]['lr'])\n",
    "        loss = self.learner.loss.detach()\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        if loss < self.min:\n",
    "            self.min = loss\n",
    "            \n",
    "        if loss > self.min*3: # stopping criteria\n",
    "            raise CancelFitException() # cancel the whole fit! Nice\n",
    "            \n",
    "        for g in self.learner.opt.param_groups:\n",
    "            g['lr'] *= self.lr_multiplier\n",
    "            \n",
    "    def plot(self):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Learning rage\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LRFinderCB()\n",
    "cbs = [lrf]\n",
    "learn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs, momentum=.85)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lrf.lrs, lrf.losses)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel(\"Learning rage\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "lrf.plot() # why not make them self-contained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c483fc",
   "metadata": {},
   "source": [
    "Increase lr over time and plot it against the loss and then we find how high we can bring the lr b4 loss gets to inf\n",
    "Lr to be chosen: 0.1.\n",
    "\n",
    "Let's now re-implement the LRFinderCB using pytorch lr schedulers, just to see that pytorch lr schedulersare actually not doing much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02deb415",
   "metadata": {},
   "source": [
    "The scheduler multipliers the lr of all params in model by a factor gamma taken as input by the scheduler. \n",
    "The multiplication occurs only when scheduler.step() is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba875bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, gamma=1.3): #gamma == lr_multiplier\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.scheduler = ExponentialLR(self.learner.opt, self.gamma)\n",
    "        self.lrs, self.losses = [], []\n",
    "        self.min = math.inf\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if not self.learner.model.training: \n",
    "            raise CancelEpochExceptionLoss()\n",
    "        \n",
    "        self.lrs.append(self.learner.opt.param_groups[0]['lr'])\n",
    "        loss = self.learner.loss.detach()\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        if loss < self.min:\n",
    "            self.min = loss\n",
    "            \n",
    "        if loss > self.min*3: # stopping criteria\n",
    "            raise CancelFitException() # cancel the whole fit! Nice\n",
    "            \n",
    "        #for g in self.learner.opt.param_groups:\n",
    "        #    g['lr'] *= self.lr_multiplier\n",
    "        self.scheduler.step()\n",
    "            \n",
    "    def plot(self):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Learning rage\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LRFinderCB()\n",
    "cbs = [lrf]\n",
    "learn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs, momentum=.85)\n",
    "learn.fit(1)\n",
    "lrf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53502d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar, master_bar\n",
    "\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self):\n",
    "        learn = self.learner\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        learn = self.learner\n",
    "        learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    \n",
    "    def after_batch(self):\n",
    "        learn = self.learner\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.model.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        learn = self.learner\n",
    "        if not learn.model.training:\n",
    "            if self.plot and hasattr(learn, 'metrics'): \n",
    "                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n",
    "                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
