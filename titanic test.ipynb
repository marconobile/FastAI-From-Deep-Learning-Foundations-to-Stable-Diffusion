{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9043d8a",
   "metadata": {},
   "source": [
    "replicate titanic example with 1 set of weights nx1\n",
    "write it with torch autograd\n",
    "gd manual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e85321a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x16c3bede590>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e488ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It contains 0 infinite values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarcoNobile\\miniconda3\\envs\\pyt\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"titanic.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "df.drop([\"PassengerId\", \"Name\", \"Cabin\"], axis=1, inplace=True)\n",
    "# create Class1 col and Class2 col where class1 = 1 iff Pclass == 1; if Pclass == 2 Class1 = 0, Class2 = 1;\n",
    "df[\"Class1\"] = 0\n",
    "df.loc[df[\"Pclass\"] == 1, \"Class1\"] = 1\n",
    "\n",
    "df[\"Class2\"] = 0\n",
    "df.loc[df[\"Pclass\"] == 2, \"Class2\"] = 1\n",
    "\n",
    "df[\"Gender\"] = 0\n",
    "df.loc[df[\"Sex\"] == \"male\", \"Gender\"] = 1\n",
    "\n",
    "df[\"Age\"] = df[\"Age\"]/df[\"Age\"].max()\n",
    "\n",
    "df[\"Fare\"] = np.log(df[\"Fare\"])\n",
    "\n",
    "df[\"SibSp1\"] = 0\n",
    "df.loc[df[\"SibSp\"] == 1, \"SibSp1\"] = 1\n",
    "\n",
    "df[\"SibSp2\"] = 0\n",
    "df.loc[df[\"SibSp\"] == 2, \"SibSp2\"] = 1\n",
    "\n",
    "df['Parch1'] = 0\n",
    "df['Parch2'] = 0\n",
    "df['Parch3'] = 0\n",
    "df.loc[(df['Parch'] == 1), 'Parch1'] = 1\n",
    "df.loc[(df['Parch'] == 2), 'Parch2'] = 1\n",
    "df.loc[(df['Parch'] == 3), 'Parch3'] = 1\n",
    "\n",
    "# C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "df['CherbourgPort'] = 0\n",
    "df['QueenstownPort'] = 0\n",
    "df.loc[(df['Embarked'] == \"C\"), 'CherbourgPort'] = 1\n",
    "df.loc[(df['Embarked'] == \"Q\"), 'QueenstownPort'] = 1\n",
    "\n",
    "# clean up residues\n",
    "df = df.drop([\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Ticket\", \"Embarked\"], axis = 1)\n",
    "\n",
    "# add bias in design matrix\n",
    "df[\"bias\"] = 1\n",
    "\n",
    "# the log creates inf values\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "count = np.isinf(df).values.sum() \n",
    "print(\"It contains \" + str(count) + \" infinite values\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c72883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(df[\"Survived\"].values, dtype=torch.double)\n",
    "df = df.drop(\"Survived\", axis=1)\n",
    "X = torch.tensor(df.values, dtype=torch.double)\n",
    "\n",
    "loss = torch.nn.MSELoss()# torch.nn.BCEWithLogitsLoss() # # CrossEntropyLoss\n",
    "relu = torch.nn.ReLU()\n",
    "lr = 0.001\n",
    "\n",
    "# Set the range boundaries\n",
    "lower_bound = -0.1\n",
    "upper_bound = 0.1\n",
    "hiddenLayerDim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39222472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0; loss=0.69\n",
      "step=1; loss=0.69\n",
      "step=2; loss=0.69\n",
      "step=3; loss=0.69\n",
      "step=4; loss=0.69\n",
      "step=5; loss=0.68\n",
      "step=6; loss=0.68\n",
      "step=7; loss=0.68\n",
      "step=8; loss=0.67\n",
      "step=9; loss=0.67\n",
      "step=10; loss=0.66\n",
      "step=11; loss=0.66\n",
      "step=12; loss=0.65\n",
      "step=13; loss=0.65\n",
      "step=14; loss=0.64\n",
      "step=15; loss=0.64\n",
      "step=16; loss=0.63\n",
      "step=17; loss=0.63\n",
      "step=18; loss=0.62\n",
      "step=19; loss=0.62\n",
      "step=20; loss=0.62\n",
      "step=21; loss=0.61\n",
      "step=22; loss=0.61\n",
      "step=23; loss=0.61\n",
      "step=24; loss=0.61\n",
      "step=25; loss=0.61\n",
      "step=26; loss=0.61\n",
      "step=27; loss=0.61\n",
      "step=28; loss=0.61\n",
      "step=29; loss=0.62\n",
      "step=30; loss=0.62\n",
      "step=31; loss=0.62\n",
      "step=32; loss=0.62\n",
      "step=33; loss=0.63\n",
      "step=34; loss=0.63\n",
      "step=35; loss=0.63\n",
      "step=36; loss=0.64\n",
      "step=37; loss=0.64\n",
      "step=38; loss=0.65\n",
      "step=39; loss=0.65\n",
      "step=40; loss=0.65\n",
      "step=41; loss=0.66\n",
      "step=42; loss=0.66\n",
      "step=43; loss=0.66\n",
      "step=44; loss=0.67\n",
      "step=45; loss=0.67\n",
      "step=46; loss=0.67\n",
      "step=47; loss=0.67\n",
      "step=48; loss=0.67\n",
      "step=49; loss=0.67\n"
     ]
    }
   ],
   "source": [
    "weightsL1 = (torch.rand(df.shape[1], dtype=torch.double) * (upper_bound - lower_bound)) + lower_bound\n",
    "weightsL1.requires_grad_()\n",
    "for i in range(50):\n",
    "    H1 = torch.matmul(X, weightsL1)    \n",
    "    H1 = relu(H1)\n",
    "    output = loss(H1, Y)     \n",
    "    output.backward()\n",
    "    with torch.no_grad(): \n",
    "        weightsL1 -= weightsL1.grad*lr        \n",
    "    print(f'step={i}; loss={output:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9db2719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nweightsL1 = (torch.rand((df.shape[1], hiddenLayerDim), dtype=torch.double) * (upper_bound - lower_bound)) + lower_bound\\nweightsL2 = (torch.rand(hiddenLayerDim, dtype=torch.double) * (upper_bound - lower_bound)) + lower_bound\\n\\nweightsL1.requires_grad_()\\nweightsL2.requires_grad_()\\n\\nfor i in range(50):\\n    H1 = torch.matmul(X, weightsL1)\\n    H1 = relu(H1)\\n    H2 = torch.matmul(H1, weightsL2)    \\n    output = loss(H2, Y)     \\n    output.backward()\\n    with torch.no_grad(): \\n        weightsL1 -= weightsL1.grad*lr\\n        weightsL2 -= weightsL2.grad*lr\\n    print(f'step={i}; loss={output:.2f}')\\n\\n    \\n# min step=49; loss=0.63\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weightsL1 = (torch.rand((df.shape[1], hiddenLayerDim), dtype=torch.double) * (upper_bound - lower_bound)) + lower_bound\n",
    "weightsL2 = (torch.rand(hiddenLayerDim, dtype=torch.double) * (upper_bound - lower_bound)) + lower_bound\n",
    "\n",
    "weightsL1.requires_grad_()\n",
    "weightsL2.requires_grad_()\n",
    "\n",
    "for i in range(50):\n",
    "    H1 = torch.matmul(X, weightsL1)\n",
    "    H1 = relu(H1)\n",
    "    H2 = torch.matmul(H1, weightsL2)    \n",
    "    output = loss(H2, Y)     \n",
    "    output.backward()\n",
    "    with torch.no_grad(): \n",
    "        weightsL1 -= weightsL1.grad*lr\n",
    "        weightsL2 -= weightsL2.grad*lr\n",
    "    print(f'step={i}; loss={output:.2f}')\n",
    "\n",
    "    \n",
    "# min step=49; loss=0.63\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b174b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch wants to know in our model class what are the tensor params -> wrap 'em in nn.Params class to make pytorch now detect \n",
    "those tensor a weights. Pytorch actually does this with all things that might be a layer of a nn -> eg nn.Linear -> this contains\n",
    "weights so they get registered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
