{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38ae1a7",
   "metadata": {},
   "source": [
    "# Learner\n",
    "The goal of the learner is to\n",
    "have a framework that allows us to tray anything very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a02c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2252d55b990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "import torch\n",
    "import pandas as pd,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import default_collate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41ea086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e18beb",
   "metadata": {},
   "source": [
    "Let's start again from fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d41140",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70284365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc7a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077f4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834b9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "tds = dsd.with_transform(transformi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982daaab",
   "metadata": {},
   "source": [
    "Now we have recreated the situation of the same notebook of ae.\n",
    "Let's see how to set up the Learner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dabc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, train_data_loader, valid_data_loader):\n",
    "        self.train = train_data_loader\n",
    "        self.valid = valid_data_loader\n",
    "\n",
    "    @classmethod # static method\n",
    "    def from_datasetDict(cls, datasetDict, batch_size): #, as_tuple=True):\n",
    "        return cls (*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds)) for ds in datasetDict.values()])\n",
    "        # this return calls __init__\n",
    "    # static method with cls allows the instanciation of the class\n",
    "    # recall that DataLoader can use multiple workers\n",
    "    # dont send anything to device here cuz huge overload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8769d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 784]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9, 1, 0, 6, 4, 3, 1, 4, 8,\n",
       "         4, 3, 0, 2, 4, 4, 5, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 32\n",
    "dls = DataLoaders.from_datasetDict(tds, bs)\n",
    "dt = dls.train\n",
    "xb, yb = next(iter(dt))\n",
    "xb.shape, yb[:bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578ea65",
   "metadata": {},
   "source": [
    "# Learner\n",
    "## First very simple approach to a generic Learner class for classification tasks\n",
    "It will replace the fit() function.\n",
    "It's main parts are:\n",
    "- fit method\n",
    "- one_epoch method\n",
    "- one_batch method\n",
    "- calc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04febdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n",
    "        \n",
    "    def one_batch(self):\n",
    "        self.xb, self.yb = to_device(self.batch)\n",
    "        self.preds = model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            self.calc_stats()\n",
    "            \n",
    "    def calc_stats(self): # over single batch\n",
    "        acc = (self.preds.argmax(dim=1)==self.yb).float().sum() \n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss*n) # takes loss not averaged over the batch\n",
    "        self.ns.append(n) # stores size of batch\n",
    "        \n",
    "    def one_epoch(self, isTrain):\n",
    "        self.model.training = isTrain\n",
    "        dl = self.dls.train if isTrain else self.dls.valid\n",
    "        for self.num, self.batch in enumerate(dl):\n",
    "            self.one_batch()\n",
    "        n = sum(self.ns) # sum of observations analyzed over multiple epochs\n",
    "        txt = \"Train step\" if isTrain else \"Validation step\"\n",
    "        print(f\"Epoch: {self.epoch}, mode: {txt}, loss: {sum(self.losses)/n}, accuracy: {sum(self.accs)/n}\")\n",
    "        # why these metrics are over multiple epochs??\n",
    "        \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs, self.losses, self.ns = [], [], [] # list of metrics over every single batch analyzed over multiple epochs\n",
    "        self.model.to(device) # device imported from lib # can't use to_device(device) here cuz it's for model!\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(isTrain=True)\n",
    "            self.one_epoch(isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfcc86e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, mode: Train step, loss: 0.5637474656105042, accuracy: 0.7900333404541016\n",
      "Epoch: 0, mode: Validation step, loss: 0.5492904782295227, accuracy: 0.7962714433670044\n",
      "Epoch: 1, mode: Train step, loss: 0.4891236126422882, accuracy: 0.8192384839057922\n",
      "Epoch: 1, mode: Validation step, loss: 0.486274778842926, accuracy: 0.8205785751342773\n",
      "Epoch: 2, mode: Train step, loss: 0.4552265703678131, accuracy: 0.8321800231933594\n",
      "Epoch: 2, mode: Validation step, loss: 0.45522287487983704, accuracy: 0.8325904607772827\n"
     ]
    }
   ],
   "source": [
    "m,nh = 28*28,50\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a5dd7",
   "metadata": {},
   "source": [
    "The problem with this learner is that it is not very flexible, eg it can be used only for classification since it uses\n",
    "a hardcoded way to compute whatever loss_func it takes in input (i.e. loss_func is always called with (xhat,y) while eg in AE\n",
    "we want (xhat,xb). Plus it computes mandatory accuracy (sensless in AE).\n",
    "\n",
    "So let's move step by step to make the Learner very flexible. Let's start to fix the metrics issue: let us create an interface/superclass for a generic Metric concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2c3f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    '''\n",
    "        Base class to be extended if particular metric is desired.\n",
    "        If not extended it computes the weighted average of its input wrt batch_size \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.vals, self.ns = [], []\n",
    "        \n",
    "    def add(self, input, target=None, batch_size=1):\n",
    "        # adds (x_hats, y) for minibatch\n",
    "        self.last = self.calc(input, target)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(batch_size)\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        ns = torch.tensor(self.ns)\n",
    "        return (torch.tensor(self.vals)*ns).sum()/ns.sum()\n",
    "    \n",
    "    def calc(self, inputs, targets): \n",
    "        ''' method to be overwritten in derived class '''\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f372eca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8824), 0.8823529411764706)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage of Metric class:\n",
    "loss = Metric() # suppose x_hat is a probability that has to be increased (supposing that last layer activation = sigmoid)\n",
    "loss.add(.9, batch_size=32)\n",
    "loss.add(.6, batch_size=2)\n",
    "\n",
    "loss.value, (.9*32 +.6*2)/(32+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91276fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    def calc(self, inputs, targets):\n",
    "        return (inputs==targets).float().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3c71909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage of Accuracy class:\n",
    "acc = Accuracy()\n",
    "\n",
    "x_hat_b1 = tensor([0,1,2,0,1]) \n",
    "y_b1 = tensor([0,0,2,1,1]) \n",
    "\n",
    "x_hat_b2 = tensor([1,1,2,0,0]) \n",
    "y_b2 = tensor([0,1,2,0,0]) \n",
    "\n",
    "acc.add(x_hat_b1, y_b1)\n",
    "acc.add(x_hat_b2, y_b2)\n",
    "\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2c205",
   "metadata": {},
   "source": [
    "# Let's now add Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad35ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class with_cbs():\n",
    "    '''\n",
    "    a callable class that is used to decorate learner methods\n",
    "    a decorator is called with the funct that it is decorating as input\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, method_name):  # method_name is what is passed in @with_cbs(\"method_name\")\n",
    "        self.method_name = method_name\n",
    "        \n",
    "    def __call__(self, f): \n",
    "        def _f(o, *args, **kwargs): # allows to forwards all inputs to original f; o its the ref to the learner\n",
    "            try:\n",
    "                o.callback(f'before_{self.method_name}') \n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'before_{self.method_name}')\n",
    "            except globals()(f'Cancel {self.method_name.title()} Exception'):\n",
    "                pass\n",
    "        return _f\n",
    "    \n",
    "# when the decorated function is called, it is CALLED/executed the function returned by the __call__ method \n",
    "# of its decorator. In with_cbs() thus:\n",
    "# - it is executed PRE a callback \n",
    "# - it is executed the original decorated function\n",
    "# - it is executed POST a callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d45d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(*args): # whatever args it are passed to this func, it returns them\n",
    "    if not args: \n",
    "        return\n",
    "    x, *args = args\n",
    "    if args:\n",
    "        return (x,)+tuple(args)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac4915e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'a', ('a', 1, ['l']))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity(1), identity(\"a\"), identity(\"a\", 1, [\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87e211f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, callbacks, opt_func=optim.SGD): \n",
    "        fc.store_attr()\n",
    "        for cb in callbacks:\n",
    "            cb.learner = self # in each callback object create a reference to this learner\n",
    "        \n",
    "    def one_batch(self):\n",
    "        self.predict()\n",
    "        self.get_loss()\n",
    "        if self.model.training:\n",
    "            self.backward()\n",
    "            self.step()\n",
    "            self.zero_grad()\n",
    "        \n",
    "    def one_epoch(self, isTrain):\n",
    "        self.model.training = isTrain\n",
    "        self.dl = self.dls.train if isTrain else self.dls.valid\n",
    "        self._one_epoch()\n",
    "        \n",
    "    def _one_epoch(self):\n",
    "        for self.iter, self.batch in enumerate(self.dl):\n",
    "            self.one_batch()\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self._fit() # actually calls -> with_cbs.__call__(self, \"fit\") # self is this learner\n",
    "    \n",
    "    @with_cbs(\"fit\")            \n",
    "    def _fit(self):\n",
    "        for self.epoch in self.epochs:\n",
    "            self.one_epoch(isTrain=True)\n",
    "            self.one_epoch(isTrain=False)\n",
    "            \n",
    "    def callback(self, method_name):\n",
    "        for cb in sorted(self.callbacks, key=attrgetter('order')):\n",
    "            getattr(cb, method_name, identity)() # if getattr does not find the method it will return identity\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a96643",
   "metadata": {},
   "source": [
    "We have a decorator and the Learner with callbacks, let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bba18566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49b61ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceCB(Callback):\n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.learner.model.to(device)\n",
    "    \n",
    "    def before_batch(self):\n",
    "        self.learner.batch = to_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "082557ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 14\u001b[0m, in \u001b[0;36mwith_cbs.__call__.<locals>._f\u001b[1;34m(o, *args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m---> 14\u001b[0m f(o, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     15\u001b[0m o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 35\u001b[0m, in \u001b[0;36mLearner._fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_epoch(isTrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_epoch(isTrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[60], line 20\u001b[0m, in \u001b[0;36mLearner.one_epoch\u001b[1;34m(self, isTrain)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;28;01mif\u001b[39;00m isTrain \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mvalid\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_one_epoch()\n",
      "Cell \u001b[1;32mIn[60], line 24\u001b[0m, in \u001b[0;36mLearner._one_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_batch()\n",
      "Cell \u001b[1;32mIn[60], line 10\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Learner' object has no attribute 'predict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cbs \u001b[38;5;241m=\u001b[39m [DeviceCB()]\n\u001b[0;32m      2\u001b[0m learner \u001b[38;5;241m=\u001b[39m Learner(model, dls, F\u001b[38;5;241m.\u001b[39mcross_entropy, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcbs)\n\u001b[1;32m----> 3\u001b[0m learner\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 30\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n_epochs)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit()\n",
      "Cell \u001b[1;32mIn[57], line 16\u001b[0m, in \u001b[0;36mwith_cbs.__call__.<locals>._f\u001b[1;34m(o, *args, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m     f(o, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     15\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCancel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod_name\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Exception\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "cbs = [DeviceCB()]\n",
    "learner = Learner(model, dls, F.cross_entropy, lr=0.2, callbacks=cbs)\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a10d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
