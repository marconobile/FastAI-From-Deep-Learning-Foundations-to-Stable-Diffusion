{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68500589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222c1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: \n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f30270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 y + 2 y_{1}$"
      ],
      "text/plain": [
       "-2*y + 2*y1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symoblic differentiation, nice\n",
    "from sympy import symbols,diff\n",
    "y1 ,y = symbols('y1 y')\n",
    "diff((y1 -y)**2, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706e24f",
   "metadata": {},
   "source": [
    "# Foundations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e5bf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max()+1 # number of classes in out\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee9f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden size\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66215d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "\n",
    "w2 = torch.randn(nh, 1) # for now use mse, out int from 0 to 9 to match on mse\n",
    "b2 = torch.zeros(1)\n",
    "# mse to simplify starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c863fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b):\n",
    "    return x@w +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98893542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_valid, w1,b1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7712c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61cc170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n",
       "        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n",
       "        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n",
       "        ...,\n",
       "        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n",
       "        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n",
       "        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fddd4",
   "metadata": {},
   "source": [
    "# First MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833856cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986cb602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade85ade",
   "metadata": {},
   "source": [
    "# Add loss function: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06d93ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5120bf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-y_valid).shape # bad cuz of broadcasting rules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043cd19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:, 0].shape # first el of all rows (which is just the only el in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dff7b432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could use squeeze() to remove ALL UNIT DIMENSIONS\n",
    "res[:, 0]; res[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f0d8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4154.01)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((res[:, 0] - y_valid)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15f29671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(yhat, y):\n",
    "    # it is better to kee the [:, 0] inside the function to avoid any preprocessing\n",
    "    # outside the func itself\n",
    "    return ((yhat[:, 0] - y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a533b4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4154.01)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(res,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b25b8",
   "metadata": {},
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17e4e7",
   "metadata": {},
   "source": [
    "Chain rule:\n",
    "suppose you have: \n",
    "    $$y=3u+9, $$ where $$ u=x^{2}$$\n",
    "then:\n",
    "$$ \\frac{dy}{dx}= \\frac{dy}{du} * \\frac{du}{dx} = 3 * 2x = 6x $$\n",
    "ChainRule baby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4cfa394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    # 4 steps l1 relu l2 mse\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2,w2,b2)\n",
    "\n",
    "def lin(inp, w, b):\n",
    "    return inp@w +b\n",
    "\n",
    "def relu(x): \n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69e0eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul wrt input\n",
    "    inp.g = out.g @ w.t() \n",
    "    \n",
    "    w.g = inp.t() @ out.g\n",
    "    b.g = out.g.sum(0) \n",
    "    # sum collapsing 0 dim since it has been broadcasted on/across cols\n",
    "    \n",
    "def forward_and_backward(inp, targ):\n",
    "\n",
    "    # forward pass    \n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass\n",
    "    # each .g accumulates all prev grads products\n",
    "    out.g = 2.* (diff[:,None] / inp.shape[0])    \n",
    "\n",
    "    lin_grad(inp=l2, out=out, w=w2, b=b2)\n",
    "    l1.g = (l1>0).float() * l2.g # relu filters gradient flow to previous layers\n",
    "    lin_grad(inp=inp, out=l1, w=w1, b=b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da42d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f9f3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes: \n",
    "# x = (50k, 784)\n",
    "# w1 = (784, 50)\n",
    "# b1 = (50) # this col vect gets broadcasted to each col of x@w1\n",
    "# l1 = x@w+b =(50k, 50) # each entry of this matrix is dot prod of\n",
    "# # an (img, col of weights) + bias \n",
    "# \n",
    "# l2 = relu(l1) # drops all entries below 0\n",
    "# l2 = (50k, 50)\n",
    "# \n",
    "# w2 = (50, 1) \n",
    "# b2 = (1)\n",
    "# \n",
    "# out = l2 @ w2 +b2 # b2 gets broadcasted to match out dims of matmul\n",
    "# out = (50k,1) + (50k,1)\n",
    "# out = (50k,1)\n",
    "# \n",
    "# out - T = (50k,1)\n",
    "# \n",
    "# then we have the loss.\n",
    "# the derivative of the mse is\n",
    "# \n",
    "# dL/dY = 2(Y-T)/N <- grad of output, shape (50k, 1)\n",
    "# \n",
    "# Y = l2 @ w2 + b2 \n",
    "# so Y depends on 3 vals: l2, w2, b2 \n",
    "# so we must take derivatives wrt all 3 vals\n",
    "# dY/dl2 = w2 <- the grad of l2\n",
    "# dY/w2 = l2 <- the grad of w2\n",
    "# dY/b2 = 1 <- the grad of b2\n",
    "# \n",
    "# but we want the grads for l2, w2, b2 wrt how they impact on the Loss\n",
    "# thus for to get the grad-impacts of l2,w2,b2 on the loss we have to \n",
    "# multiply each one of [dY/dl2, dY/w2, dY/b2] by dL/dY <- this is the grad of\n",
    "# the output, it has already been computed in backprop -> we need to take out\n",
    "# as input aswell to multiply it for the reason just described.\n",
    "# \n",
    "# each col of w_i is a set of weights used to model/map all input obs into another\n",
    "# dimensional space + bias/eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa70a62",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c00a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the gradients s.t. test our code\n",
    "def get_grad(x):\n",
    "    return x.g.clone()\n",
    "\n",
    "# this is a cool way to define els inside list and list at once\n",
    "chks = w1,w2,b1,b2, x_train\n",
    "grads = w1_g, w2_g,b1_g,b2_g, inp_g = map(get_grad, chks)\n",
    "\n",
    "\n",
    "def mkgrad(x):\n",
    "    return x.clone().requires_grad_(True)\n",
    "\n",
    "ptgrads = w1_copy, w2_copy, b1_copy, b2_copy, xt_copy = map(mkgrad, chks)\n",
    "\n",
    "def forward(inp, targets):\n",
    "    l1 = lin(inp, w1_copy, b1_copy)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2_copy, b2_copy)\n",
    "    return mse(out, targets)\n",
    "\n",
    "loss = forward(xt_copy, y_train)\n",
    "loss.backward()\n",
    "\n",
    "for a,b in zip(grads, ptgrads):\n",
    "    test-close(a.grad, b, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd7883",
   "metadata": {},
   "source": [
    "# Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2064bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recalling that by modifiying an object that is stored in other classes implies that classes contnent is changed\n",
    "\n",
    "class A():\n",
    "    def __init__(self, data): self.x = data\n",
    "        \n",
    "class B():\n",
    "    def __init__(self, data): self.x = data\n",
    "        \n",
    "class  C():\n",
    "    def __init__(self, data): self.x = data\n",
    "\n",
    "c = C(8) \n",
    "a = A(c)\n",
    "b = B(c)\n",
    "print(a.x.x, b.x.x)\n",
    "c.x = 9\n",
    "a.x.x, b.x.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82a4bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        # it stores its own input, output\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.) \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        # self.imp.g = relu's derivative  * chainRule: \n",
    "            # all the grad up untill now in chain\n",
    "        self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cb08448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(self.inp, self.w, self.b)\n",
    "        return self.out \n",
    "    \n",
    "    def backward(self):\n",
    "        # when calling backward for the first time\n",
    "        # self.out.g has been already modified: contains the grad computed by Mse.backward()\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8115377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, target):\n",
    "        # inp is the out of prev layer\n",
    "        self.inp, self.target = inp, target\n",
    "        self.out = mse(self.inp, self.target)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2*(self.inp.squeeze() - self.target).unsqueeze(-1) / self.target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082d23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, w2, b1, b2):\n",
    "        self.layers = [LinearLayer(w1, b1), Relu(), LinearLayer(w2, b2)] \n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, target):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): # from last layer to first layer!\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14fe83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, w2, b1, b2)\n",
    "model(x_train, y_train)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6417e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w1_g, w1.g, eps=0.01)\n",
    "test_close(w2_g, w2.g, eps=0.01)\n",
    "test_close(b1_g, b1.g, eps=0.01)\n",
    "test_close(b2_g, b2.g, eps=0.01)\n",
    "test_close(inp_g, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd28038",
   "metadata": {},
   "source": [
    "# Further Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e9a90",
   "metadata": {},
   "source": [
    "As we can see above, each class that we created above, in their __call__ method there is repeated calls: self.in = in; compute out and return self.out...\n",
    "\n",
    "All backward calls need self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad99c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args # store all things that are passed in a list\n",
    "        self.out = self.forward(*args) # calls derived forward()\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): \n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "    def backward(self):\n",
    "        # All backward calls need self.out to apply\n",
    "        # chainRule: all the grad up untill now in chain\n",
    "        # this takes *self.args that is particular for each derived forward() signature \n",
    "        self.bwd(self.out, *self.args) # list unpacking\n",
    "\n",
    "    def bwd(self):\n",
    "        raise Exception(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9dc24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module): # inherit Module \"interface\" (nn.Module in pytorch)\n",
    "\n",
    "    # now coder is required to implemnt only \n",
    "    # forward and how to get derivative of this forward * gradient up untill now (out.g); much cleaner\n",
    "    \n",
    "    def forward(self, inp): \n",
    "        '''\n",
    "        example usage:\n",
    "        relu = Relu()\n",
    "        x1 = relu(x)\n",
    "        '''\n",
    "        return inp.clamp_min(0.)\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e58e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91c79d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward(self, inp, targets):\n",
    "        self.diff = inp.squeeze() - targets\n",
    "        return self.diff.pow(2).mean()\n",
    "        \n",
    "    def bwd(self, _, inp, targets): # _ = out\n",
    "        inp.g = 2. * self.diff.unsqueeze(-1) /targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97cde0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, w2, b1, b2)\n",
    "model(x_train, y_train)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e226e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w1_g, w1.g, eps=0.01)\n",
    "test_close(w2_g, w2.g, eps=0.01)\n",
    "test_close(b1_g, b1.g, eps=0.01)\n",
    "test_close(b2_g, b2.g, eps=0.01)\n",
    "test_close(inp_g, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f50c6",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "So now we have built on our own derivatives computation for layers. \n",
    "So we can now use autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71d15586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f351d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8149af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in, nh), nn.ReLU(), Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x, targets):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return F.mse_loss(x, targets[:,None].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c07d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(784, 50, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
