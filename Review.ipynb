{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f125d11",
   "metadata": {},
   "source": [
    "# From Deep Learning Foundations to Stable Diffusion\n",
    "\n",
    "### Highlights for study:\n",
    "#### Lecture 2\n",
    "- Path object\n",
    "- generators/islice -> use generator instead of lists\n",
    "- \\_\\_getitem\\_\\_(self, idxs) -> [] overload\n",
    "- map -> apply f to iterable\n",
    "- globality of random state\n",
    "- torch.unique; torch.bincount\n",
    "\n",
    "#### Lecture 3\n",
    "- partial (funcs with partial input args), equivalent to 2 lambdas (lect5c)\n",
    "\n",
    "#### Lecture 4\n",
    "- nn.Module\n",
    "\n",
    "#### Lecture 5a\n",
    "- hasattr; works with class members/methods\n",
    "- \\_\\_setattr\\_\\_ -> creates attribute dynamically\n",
    "- reduce\n",
    "- \\_\\_iter\\_\\_ + next(iter(x))\n",
    "- yield from \n",
    "- dataset\n",
    "- collate_fn\n",
    "- dataloader\n",
    "\n",
    "#### Lecture 5b\n",
    "- @inplace\n",
    "- itemgetter\n",
    "- ig = itemgetter('a', 'c') # returns a func that gets vals form dict at keys 'a' and 'c'; call: ig(dict)\n",
    "- default_collate: input: 2 (or more) dicts with k keys -> returns {k:k, v: stacks together values for same keys across input dicts}\n",
    "\n",
    "#### Lecture 5c\n",
    "- callbacks: a callable that will calld back to when smthing happens\n",
    "- partial (funcs with partial input args), equivalent to 2 lambdas\n",
    "- Callable Classes: \\_\\_call\\_\\_ used to store state/info across calls\n",
    "- getattr(self.cb,cb_name, None) # from callback obj select method\n",
    "- \\_\\_getattr\\_\\_(self, k,v) used when call: foo.x: k=x, v=foo.x; it is only called if the member does not exists \n",
    "\n",
    "#### Lecture6\n",
    "- [[(i,j) for j in range(N)] for i in range(N)] # structs coords grid\n",
    "- to_device\n",
    "\n",
    "#### Lecture7\n",
    "- @classmethod, alternative ctor\n",
    "- fc.store_attr()\n",
    "- @property -> calls method without parenthesis, the method must do not take inputs (only self)\n",
    "\n",
    "#### Lecture8\n",
    "- torch metrics: https://torchmetrics.readthedocs.io/en/stable/pages/quickstart.html\n",
    "- detach\n",
    "- ...todo\n",
    "\n",
    "#### Lecture8b\n",
    "- decorators detail\n",
    "- callbacks\n",
    "- @property\n",
    "- lrfinder\n",
    "- momentum\n",
    "\n",
    "#### Lecture8c\n",
    "- model diagnostic\n",
    "- lr idea/analysis on its abs value\n",
    "- scheduler\n",
    "\n",
    "#### Lecture9a INITIALIZATION IMPO\n",
    "- model diagnostic\n",
    "- lr idea/analysis on its abs value\n",
    "- IMPO: on w init and normalizations\n",
    "- IMPO: reasoning on act func generalRelu\n",
    "- You need to init your net correctly wrt your activation function!\n",
    "\n",
    "#### Lecture9b\n",
    "- optimizers momentum rmsprop adam  \n",
    "- api exploration \n",
    "- schedulers\n",
    "- OneCycleLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d5aab",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bf92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i): # overloader for [] operator, supports slicing\n",
    "        # preprocessing, usually is parallelized\n",
    "        # dont send anything to device here cuz huge overload if multiple worker\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced1fd1",
   "metadata": {},
   "source": [
    "# Collate fnc (optionally) used in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate(b): # b = [(xi,yi), (xj,yj), ...]\n",
    "    # preprocessing, usually is parallelized\n",
    "    # dont send anything to device here cuz huge overload if multiple worker\n",
    "    xs, ys = zip(*b) # creates 2 lists with all xis and all yis\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db1905",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aceafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "bs = bs if train else if valid 2*bs\n",
    "shuffle = True if train if valid False\n",
    "\n",
    "dl = DataLoader(dataset_obj, bs, shuffle=shuffle, drop_last=?, num_workers, collate_fn = collate_fn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e955d4",
   "metadata": {},
   "source": [
    "# Decorators and inplace decorator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeac9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make an implace func have a return \n",
    "def inplace(f): # input: a func, returns a func\n",
    "    def _f(b): # defs a new func\n",
    "        f(b) # applies the input func that acts on its input inplace\n",
    "        return b # return the modified input\n",
    "    return _f # return a func that modifies b inplace\n",
    "\n",
    "# @ before a func: take the func defd here below, pass it as input to the @func, replace the func defd here below with the func returned by the @func\n",
    "@inplace # name of the decorating func\n",
    "def transform_(b): \n",
    "    b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c5e6f",
   "metadata": {},
   "source": [
    "# ToDevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from typing import Mapping\n",
    "def to_device(x, device=device): \n",
    "    if isinstance(x, Mapping): # i.e. if it is a dict -> if isinstance(x, dict):\n",
    "        return {k:v.to(device) for k,v in x.items()} # sends each val to device\n",
    "    return type(x)(o.to(device) for o in x)    \n",
    "\n",
    "'''\n",
    "Mapping: A container object that supports arbitrary key lookups and implements the methods \n",
    "    specified in the collections.abc.Mapping or collections.abc.MutableMapping abstract base classes. \n",
    "    Examples include dict, collections.defaultdict, collections.OrderedDict and collections.Counter.\n",
    "'''\n",
    "\n",
    "'''\n",
    "type(x)(...): Creates a new object of the same type as x. \n",
    "    It's equivalent to calling the constructor of the type with the arguments inside the parentheses.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
